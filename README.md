# LoRA & DoRA in TinyGrad

This project demonstrates how to implement and apply **LoRA (Low-Rank Adaptation)** and **DoRA (Direct Output Rank Adaptation)** techniques in [TinyGrad](https://github.com/geohot/tinygrad). These methods allow efficient fine-tuning of deep learning models by injecting low-rank adapters into linear layers.

---

## ðŸ“Š View Computation Graph

To view the computation graph of the model:

```bash
GRAPH=1 ./test.py
```

---

## ðŸ§  What are LoRA & DoRA?

- **LoRA** allows you to fine-tune only a small number of trainable parameters by inserting low-rank matrices into pre-trained models, preserving the original weights.
- **DoRA** is a more recent technique that directly adapts the output ranks for efficient fine-tuning with minimal overhead.

Watch [Miss Coffee Bean's video](https://www.youtube.com/@misscoffeebean) for a friendly explanation and motivation behind these approaches.

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ dora_tinygrad/           # DoRA implementation
â”‚   â””â”€â”€ modules/             # Base and linear module classes
â”œâ”€â”€ lora_tinygrad/           # LoRA implementation
â”‚   â””â”€â”€ modules/             # Base and linear module classes
â”œâ”€â”€ examples/                # Example scripts and utils
â”‚   â”œâ”€â”€ example_lora.py      # End-to-end LoRA training + finetuning
â”‚   â”œâ”€â”€ example_dora.py      # End-to-end DoRA training + finetuning
â”‚   â”œâ”€â”€ mnist_example.ipynb  # Notebook to play with MNIST + LoRA
â”‚   â”œâ”€â”€ test_lora.py         # Graph/debugging script for LoRA
â”‚   â””â”€â”€ utils.py             # Training, evaluation, misc helpers
â”œâ”€â”€ test.py                  # Entry point for testing LoRA/DoRA
â””â”€â”€ README.md                # You're here!
```

---

## ðŸš€ How to Run

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/tinygrad-lora-dora
cd tinygrad-lora-dora
```

### 2. Set up Virtual Environment (Recommended)

```bash
python -m venv .env
source .env/bin/activate
```

> Jupyter issues? Run: `ipython kernel install --name "local-venv-kernel" --user`

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸ§ª Try the Examples

### Run LoRA Example:

```bash
python examples/example_lora.py
```

### Run DoRA Example:

```bash
python examples/example_dora.py
```

---

## ðŸ“˜ Notes

- This project **does not use external libraries** like `peft`, `transformers`, or `accelerate`. It is meant to be educational and minimal.
- TinyGrad is a great environment to understand low-level ML concepts. We leverage this simplicity to explain and explore LoRA and DoRA directly in the core logic.

---

## ðŸ“„ References

- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [DoRA Paper](https://arxiv.org/abs/2402.09353)
- [TinyGrad by George Hotz (geohot)](https://github.com/geohot/tinygrad)

---

## ðŸ§Š License

MIT License. See `LICENSE` file for details.

---

Happy fine-tuning with less memory! ðŸŽ‰
