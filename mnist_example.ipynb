{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5443aea-e1ce-4667-9a83-ff6a7482b75f",
   "metadata": {},
   "source": [
    "# Lora finetuning\n",
    "> Example of finetuning lora\n",
    "\n",
    "In the following notebook we are going to use my custom implementation of LoRA to fine-tune a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28726d2-8a8f-46f8-b734-54c9d28e77a7",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780e6273-af80-4264-a68f-82a4ed80e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tinygrad import Tensor, nn\n",
    "import copy\n",
    "\n",
    "# from extra.training import evaluate, train\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98afb72-3e0c-4c52-bbc5-d42e8f700afb",
   "metadata": {},
   "source": [
    "##### Importing custom LoRA library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f58925-8a6c-4676-abe6-47004e8cb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_tinygrad import LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8ceef-b83a-4ed9-ace0-88c14bcf5b8a",
   "metadata": {},
   "source": [
    "### Define a simple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b591ab9a-2058-4d2c-a111-e69ae2932ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet:\n",
    "    def __init__(self):\n",
    "        self.l1 = nn.Linear(784, 784 * 3, bias=False)\n",
    "        self.l2 = nn.Linear(784 * 3, 784, bias=False)\n",
    "        self.l3 = nn.Linear(784, 128, bias=False)\n",
    "        self.l4 = nn.Linear(128, 10, bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.l1(x).leakyrelu()\n",
    "        x = self.l2(x).leakyrelu()\n",
    "        x = self.l3(x).leakyrelu()\n",
    "        x = self.l4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab64643-011b-4daf-862e-6cf6e4bd13a1",
   "metadata": {},
   "source": [
    "## Model pre-training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e2ddd-de04-4d69-ade5-6e1fce547a7a",
   "metadata": {},
   "source": [
    "#### Hyperparameters & Fetching Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ceae125-1350-47f8-81b3-4a49751ba15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochss = 3\n",
    "BS = 128\n",
    "n_outputs = 10\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = fetch_fashion_mnist()\n",
    "steps = len(X_train) // BS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73467d72-f067-4ccd-8d4f-65a5aebb7100",
   "metadata": {},
   "source": [
    "#### Defining the model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0d7a60-f675-419a-823c-2fed4c22992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = TinyNet()\n",
    "\n",
    "# Define loss function\n",
    "lossfn = Tensor.sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a5359-c365-4589-9d3e-7af64429b226",
   "metadata": {},
   "source": [
    "#### Traning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fa856c-18e6-4462-bcb1-69bd52520c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.43 accuracy 0.84: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:04<00:00, 100.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 205.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy is 0.841000\n",
      "reducing lr to 0.0008333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.29 accuracy 0.88: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:04<00:00, 110.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 185.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy is 0.860400\n",
      "reducing lr to 0.0006944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.32 accuracy 0.88: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 468/468 [00:04<00:00, 110.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 190.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy is 0.868700\n",
      "reducing lr to 0.0005787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-training the model\n",
    "for _ in range(epochss):\n",
    "    optimizer = nn.optim.Adam(nn.state.get_parameters(model), lr=lr)\n",
    "    train(model, X_train, Y_train, optimizer, lossfn=lossfn, steps=steps, BS=BS)\n",
    "    accuracy, Y_test_pred = evaluate(model, X_test, Y_test, return_predict=True)\n",
    "    lr /= 1.2\n",
    "    print(f\"reducing lr to {lr:.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d981a-71f8-4a27-8e9f-a16ea7a6edda",
   "metadata": {},
   "source": [
    "#### Get mislabeled predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62000b49-7f87-4702-9b40-35701ef618de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Missing 186\n",
      "Class 1: Missing 37\n",
      "Class 2: Missing 230\n",
      "Class 3: Missing 125\n",
      "Class 4: Missing 160\n",
      "Class 5: Missing 85\n",
      "Class 6: Missing 344\n",
      "Class 7: Missing 67\n",
      "Class 8: Missing 45\n",
      "Class 9: Missing 34\n",
      "Worst class: 6\n"
     ]
    }
   ],
   "source": [
    "mislabeled_counts = get_mislabeled_counts(Y_test, Y_test_pred, n_output=n_outputs)\n",
    "pretty_print_mislabeled_counts(mislabeled_counts)\n",
    "\n",
    "worst_class = max(mislabeled_counts, key=lambda k: mislabeled_counts[k])\n",
    "print(f\"Worst class: {worst_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d36961-040d-4df0-b34d-b82787b5cd68",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Let's start by craeting a dataset for the finetuning on the worst examples to see if there is actually some improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd49c4d-794e-4e87-b4e1-bda2ee0c1ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the worst class, 6..\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m BS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get a mixture which is mostly filled with the worst class\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mmix_old_and_new_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworst_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BS\n",
      "File \u001b[0;32m~/Github/ML_Project_LoRA/utils.py:38\u001b[0m, in \u001b[0;36mmix_old_and_new_data\u001b[0;34m(X, Y, worst_class, ratio)\u001b[0m\n\u001b[1;32m     35\u001b[0m worst_class_x, worst_class_y \u001b[38;5;241m=\u001b[39m filter_data_by_class(X, Y, worst_class)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Randomly sample from the worst class\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m worst_class_x, worst_class_y \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworst_class_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworst_class_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworst_class_samples\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Combine the worst class samples with the rest of the data\u001b[39;00m\n\u001b[1;32m     43\u001b[0m mixed_X \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/random.py:430\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    428\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    432\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "print(f\"Fine-tuning the worst class, {worst_class}..\")\n",
    "lrs = 1e-5\n",
    "epochss = 1\n",
    "BS = 64\n",
    "\n",
    "# Get a mixture which is mostly filled with the worst class\n",
    "X_train, Y_train = mix_old_and_new_data(X_train, Y_train, worst_class, ratio = 0.5)\n",
    "steps = len(X_train) // BS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce446f4-0cac-4f7a-aa13-c3df86a7b1c8",
   "metadata": {},
   "source": [
    "### Fine-tuning without Lora\n",
    "\n",
    "Let's first do a full finetuning of the model to then compare the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a8e7c-bb56-4010-97f8-b7f665d37b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the model\n",
    "model_full_finetuning = copy.deepcopy(model) \n",
    "\n",
    "# Finetuning the model\n",
    "for _ in range(epochss):\n",
    "    optimizer = nn.optim.Adam(nn.state.get_parameters(model_full_finetuning), lr=lr)\n",
    "    # Default loss function is sparse_categorical_crossentropy\n",
    "    train(model_full_finetuning, X_train, Y_train, optimizer, steps=steps, BS=BS)\n",
    "    accuracy, Y_test_pred = evaluate(model_full_finetuning, X_test, Y_test, return_predict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a72ffac-e39e-4f5e-a6ca-21246c021d2f",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c640f47-99da-45c4-9fc5-7426daaf5ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mislabeled_counts = get_mislabeled_counts(Y_test, Y_test_pred, n_output=n_outputs)\n",
    "pretty_print_mislabeled_counts(mislabeled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2306ba-4a8b-4bb9-b694-0af7865123ad",
   "metadata": {},
   "source": [
    "### Fine-tuning with Lora\n",
    "\n",
    "Now let's do the Lora finetuning on the other same data with a rank of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45b782-d8be-4ec5-a09e-c2b2ba7b4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Lora model from the original model without modifying the original one\n",
    "lora_model = LoRA.from_module(model, rank=16, inplace=False)\n",
    "\n",
    "# Pre-training the model\n",
    "for _ in range(epochss):\n",
    "    optimizer = nn.optim.Adam(lora_model.parameters(), lr=lr)\n",
    "    # Default loss function is sparse_categorical_crossentropy\n",
    "    train(lora_model, X_train, Y_train, optimizer, steps=steps, BS=BS)\n",
    "    accuracy, Y_test_pred = evaluate(lora_model, X_test, Y_test, return_predict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c86e00-f2f8-49fe-9a7e-bbe707c3668b",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904d40c-b096-47cc-9509-9cf9962fac41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mislabeled_counts = get_mislabeled_counts(Y_test, Y_test_pred, n_output=n_outputs)\n",
    "pretty_print_mislabeled_counts(mislabeled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef5189-90b4-4928-be3a-4bcea22c9708",
   "metadata": {},
   "source": [
    "#### Show the parameters we trained in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b1ec5-df31-4188-8010-4b1e25bb5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_parameters = sum(p.numel() for p in nn.state.get_parameters(model_full_finetuning))\n",
    "lora_parameters = sum(p.numel() for p in lora_model.parameters())\n",
    "\n",
    "print(f\"{original_parameters = }\")\n",
    "print(f\"{lora_parameters = }\")\n",
    "print(f\"Percentage of parameters we update: {(lora_parameters / original_parameters) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2592d-bd7c-4ad2-8436-fb296c9b1c20",
   "metadata": {},
   "source": [
    "## Other functionalities\n",
    "\n",
    "In the following section we will test some other functionalities I implemented in the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae80bcd-d545-4d50-890a-582bfc5176e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a random example to test the model\n",
    "x = Tensor.randn(1, 28, 28).reshape(-1)\n",
    "\n",
    "# Assert if the values are not all the same and thus I have done something\n",
    "assert not np.allclose(model(x).numpy(), lora_model(x).numpy()), \"The outputs are too close!\"\n",
    "\n",
    "# Disable the lora parameters\n",
    "lora_model.disable_lora()\n",
    "\n",
    "# Assert if the values are the same and thus I haven't changed the original model\n",
    "assert np.allclose(model(x).numpy(), lora_model(x).numpy()), \"The outputs are too close!\"\n",
    "print(\"Everything works as expected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
